{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc6dcde-5bc5-4199-b3ef-a3b780f17c0e",
   "metadata": {},
   "source": [
    "Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "\n",
    "A machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data.\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise\n",
    "Consequences:-\n",
    "\n",
    "The underfitting model has High bias and low variance.\n",
    "It can be increased by:-\n",
    "Increase model complexity.\n",
    "Increase the number of features, performing feature engineering.\n",
    "Remove noise from the data.\n",
    "Increase the number of epochs or increase the duration of training to get better results\n",
    "\n",
    "\n",
    "Overfitting has low bias and high variance.It is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
    "It can be increased by:-\n",
    "Increase training data.\n",
    "Reduce model complexity.\n",
    "Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "Ridge Regularization and Lasso Regularization.\n",
    "Use dropout for neural networks to tackle overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4177bc-41bc-445d-ab8f-96ca62e5fd71",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies. Early stopping pauses the training phase before the machine learning model learns the noise in the data.\n",
    "Identify several features or parameters that impact the final prediction when you build a model. Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84151d-d968-4dae-a26b-471dc1072b30",
   "metadata": {},
   "source": [
    "Q3. Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "\n",
    "An underfitted model has high bias and low variance.\n",
    "\n",
    "Scenarios of underfitting are:-\n",
    "Data used for training is not cleaned and contains noise (garbage values) in it.\n",
    "The model has a high bias.\n",
    "The size of the training dataset used is not enough.\n",
    "The model is too simple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16b015-4210-4869-a3b6-9af91cd58c31",
   "metadata": {},
   "source": [
    "4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time. \n",
    "Bias-Variance tradeoff helps optimize the error in our model and keeps it as low as possible. An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data. In this, both the bias and variance should be low so as to prevent overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a70669-3a83-4c12-a7f5-09896b7af7cd",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "We can identify if a machine learning model has overfit by first evaluating the model on the training dataset and then evaluating the same model on a holdout test dataset.\n",
    "If the performance of the model on the training dataset is significantly better than the performance on the test dataset, then the model may have overfit the training dataset.\n",
    "We care about overfitting because it is a common cause for “poor generalization” of the model as measured by high “generalization error.” That is error made by the model when making predictions on new data.\n",
    "A plot of the model performance on the train and test set can be calculated at each point during training and plots can be created. This plot is often called a learning curve plot, showing one curve for model performance on the training set and one curve for the test set for each increment of learning.\n",
    "The common pattern for overfitting can be seen on learning curve plots, where model performance on the training dataset continues to improve (e.g. loss or error continues to fall or accuracy continues to rise) and performance on the test or validation set improves to a point and then begins to get worse\n",
    "One approach for performing an overfitting analysis on algorithms that do not learn incrementally is by varying a key model hyperparameter and evaluating the model performance on the train and test sets for each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e3bb4-1e07-468f-a55f-3af33b4e2f40",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "Bias \n",
    "Bias occurs in a machine learning model when an algorithm is used but does not fit properly. \t.\n",
    "It is the difference between the actual values and the predicted values. \t \t\n",
    "\n",
    "Variance\n",
    "Variance is the amount of variation the target function estimation will change if different training data is used.\n",
    "It talks about how much any random variable deviated from the expected value.\n",
    "\n",
    "Characteristics of a high-bias model are:\n",
    "Potential of underfitting \n",
    "Unable to capture accurate data trends\n",
    "Extremely simplified\n",
    "High error rate\n",
    "\n",
    "\n",
    "Characteristics of a high variance model are:\n",
    "Potential of overfitting\n",
    "Noise in the data set\n",
    "Trying to fit all data points as close as possible \n",
    "Complex models\n",
    "\n",
    "High Bias - High Variance: Predictions are inconsistent and inaccurate on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c6270-6055-443d-a061-e8edbcdd6fe0",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it\n",
    "\n",
    "\n",
    "The commonly used regularization techniques are : \n",
    "Lasso Regularization – L1 Regularization\n",
    "Ridge Regularization – L2 Regularization\n",
    "Elastic Net Regularization – L1 and L2 Regularization\n",
    "\n",
    "A regression model which uses the L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. Lasso Regression adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function(L). Lasso regression also helps us achieve feature selection by penalizing the weights to approximately equal to zero if that feature does not serve any purpose in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993f91a-7e74-4a53-b901-922c07047e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
